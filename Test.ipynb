{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steel/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated, Literal, Dict, Union, Optional \n",
    "from datetime import datetime\n",
    "\n",
    "from tavily import AsyncTavilyClient, TavilyClient\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AnyMessage, AIMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, conlist\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END, add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font(\"Arial\", \"B\", 12)\n",
    "        self.cell(0, 10, \"\", 0, 1, \"C\")\n",
    "\n",
    "    def footer(self):\n",
    "        self.set_y(-15)\n",
    "        self.set_font(\"Arial\", \"I\", 8)\n",
    "        self.cell(0, 10, f\"Page {self.page_no()}\", 0, 0, \"C\")\n",
    "\n",
    "def sanitize_content(content):\n",
    "    try:\n",
    "        # Use 'utf-8' encoding to handle Unicode characters\n",
    "        encoded_content = content.encode('utf-8', 'ignore').decode('utf-8')\n",
    "        return encoded_content\n",
    "    except UnicodeEncodeError as e:\n",
    "        print(f\"Encoding error: {e}\")\n",
    "\n",
    "        # Remove problematic characters using 'ascii' encoding\n",
    "        sanitized_content = content.encode('ascii', 'ignore').decode('ascii')\n",
    "        return sanitized_content\n",
    "\n",
    "def replace_problematic_characters(content):\n",
    "    # Replace or remove problematic characters\n",
    "    replacements = {\n",
    "        '\\u2013': '-',  # en dash to hyphen\n",
    "        '\\u2014': '--',  # en dash to double hyphen\n",
    "        '\\u2018': \"'\",  # left single quotation mark to apostrophe\n",
    "        '\\u2019': \"'\",  # right single quotation mark to apostrophe\n",
    "        '\\u201c': '\"',  # left double quotation mark to double quote\n",
    "        '\\u201d': '\"',  # right double quotation mark to double quote\n",
    "        '\\u2026': '...',  # horizontal ellipsis\n",
    "        '\\u2010': '-',   # dash\n",
    "        '\\u2022': '*',   # bullet\n",
    "        '\\u2122': 'TM'  # TradeMark Symbol\n",
    "    }\n",
    "\n",
    "    for char, replacement in replacements.items():\n",
    "        content = content.replace(char, replacement)\n",
    "\n",
    "    return content\n",
    "\n",
    "def generate_pdf_from_md(content, filename='output.pdf'):\n",
    "    try:\n",
    "        pdf = PDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "        pdf.set_font('Arial', '', 12)\n",
    "\n",
    "        sanitized_content = sanitize_content(content)\n",
    "        sanitized_content = replace_problematic_characters(sanitized_content)\n",
    "\n",
    "        lines = sanitized_content.split('\\n')\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith('#'):\n",
    "                header_level = min(line.count('#'), 4)\n",
    "                header_text = re.sub(r'\\*{2,}', '', line.strip('# ').strip())\n",
    "                pdf.set_font('Arial', 'B', 12 + (4 - header_level) * 2)\n",
    "                pdf.multi_cell(0, 10, header_text)\n",
    "                pdf.set_font('Arial', '', 12)\n",
    "            else:\n",
    "                parts = re.split(r'(\\*\\*\\*.*?\\*\\*\\*|\\*\\*.*?\\*\\*|\\*.*?\\*|\\[.*?\\]\\(.*?\\)|\\([^ ]+?\\))', line)\n",
    "                for part in parts:\n",
    "                    if re.match(r'\\*\\*\\*.*?\\*\\*\\*', part):  # Bold Italic\n",
    "                        text = part.strip('*')\n",
    "                        pdf.set_font('Arial', 'BI', 12)\n",
    "                        pdf.write(10, text)\n",
    "                    elif re.match(r'\\*\\*.*?\\*\\*', part):  # Bold\n",
    "                        text = part.strip('*')\n",
    "                        pdf.set_font('Arial', 'B', 12)\n",
    "                        pdf.write(10, text)\n",
    "                    elif re.match(r'\\*.*?\\*', part):  # Italic\n",
    "                        text = part.strip('*')\n",
    "                        pdf.set_font('Arial', 'I', 12)\n",
    "                        pdf.write(10, text)\n",
    "                    elif re.match(r'\\[.*?\\]\\(.*?\\)', part):  # Markdown-style link\n",
    "                        display_text = re.search(r'\\[(.*?)\\]', part).group(1)\n",
    "                        url = re.search(r'\\((.*?)\\)', part).group(1)\n",
    "                        pdf.set_text_color(0, 0, 255)  # Set text color to blue\n",
    "                        pdf.set_font('', 'U')\n",
    "                        pdf.write(10, display_text, url)\n",
    "                        pdf.set_text_color(0, 0, 0)  # Reset text color\n",
    "                        pdf.set_font('Arial', '', 12)\n",
    "                    # elif re.match(r'\\([^ ]+?\\)', part):  # Plain URL\n",
    "                    #     url = part[1:-1]\n",
    "                    #     pdf.set_text_color(0, 0, 255)  # Set text color to blue\n",
    "                    #     pdf.set_font('', 'U')\n",
    "                    #     pdf.write(10, url, url)\n",
    "                    else:\n",
    "                        pdf.write(10, part)\n",
    "                    pdf.set_text_color(0, 0, 0)             # Reset text color\n",
    "                    pdf.set_font('Arial', '', 12)   # Reset font\n",
    "\n",
    "                pdf.ln(10)\n",
    "\n",
    "        pdf.output(filename)\n",
    "        return f\"PDF generated: {filename}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating PDF: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the research state\n",
    "class ResearchState(TypedDict):\n",
    "    company: str\n",
    "    company_keywords: str\n",
    "    exclude_keywords: str\n",
    "    report: str\n",
    "    # Declare a dictionary where:\n",
    "    # - The outer dictionary has string keys.\n",
    "    # - The inner dictionary can have keys of different types (e.g., str, int).\n",
    "    # - The inner dictionary values can be of different types (e.g., str, float).\n",
    "    documents: Dict[str, Dict[Union[str, int], Union[str, float]]]\n",
    "    RAG_docs: Dict[str, Dict[Union[str, int], Union[str, float]]]\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# Define the structure for the model's response, which includes citations.\n",
    "class Citation(BaseModel):\n",
    "    source_id: str = Field(\n",
    "        ...,\n",
    "        description=\"The url of a SPECIFIC source which justifies the answer.\",\n",
    "    )\n",
    "    quote: str = Field(\n",
    "        ...,\n",
    "        description=\"The VERBATIM quote from the specified source that justifies the answer.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QuotedAnswer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The answer to the user question, which is based only on the given sources. Include any relevant sources in the answer as markdown hyperlinks. For example: 'This is a sample text ([url website](url))'\"\n",
    "    )\n",
    "    citations: List[Citation] = Field(\n",
    "        ..., description=\"Citations from the given sources that justify the answer.\"\n",
    "    )\n",
    "    \n",
    "# Add Tavily's arguments to enhance the web search tool's capabilities\n",
    "class TavilyQuery(BaseModel):\n",
    "    query: str = Field(description=\"web search query\")\n",
    "    topic: str = Field(description=\"type of search, should be 'general' or 'news'. Choose 'news' ONLY when the company you searching is publicly traded and is likely to be featured on popular news\")\n",
    "    days: int = Field(description=\"number of days back to run 'news' search\")\n",
    "    # raw_content: bool = Field(description=\"include raw content from found sources, use it ONLY if you need more information besides the summary content provided\")\n",
    "    domains: Optional[List[str]] = Field(default=None, description=\"list of domains to include in the research. Useful when trying to gather information from trusted and relevant domains\")\n",
    " \n",
    "\n",
    "# Define the args_schema for the tavily_search tool using a multi-query approach, enabling more precise queries for Tavily.\n",
    "class TavilySearchInput(BaseModel):\n",
    "    sub_queries: List[TavilyQuery] = Field(description=\"set of sub-queries that can be answered in isolation\")\n",
    "\n",
    "\n",
    "class TavilyExtractInput(BaseModel):\n",
    "    urls: List[str] = Field(description=\"list of a single or several URLs for extracting raw content to gather additional information\")\n",
    "\n",
    "\n",
    "@tool(\"tavily_search\", args_schema=TavilySearchInput, return_direct=True)\n",
    "async def tavily_search(sub_queries: List[TavilyQuery]):\n",
    "    \"\"\"Perform searches for each sub-query using the Tavily search tool concurrently.\"\"\"  \n",
    "    # Define a coroutine function to perform a single search with error handling\n",
    "    async def perform_search(itm):\n",
    "        try:\n",
    "            # Add date to the query as we need the most recent results\n",
    "            query_with_date = f\"{itm.query} {datetime.now().strftime('%m-%Y')}\"\n",
    "            # Attempt to perform the search, hardcoding days to 7 (days will be used only when topic is news)\n",
    "            response = await tavily_client.search(query=query_with_date, topic=itm.topic, days=itm.days, max_results=10)\n",
    "            return response['results']\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions, log them, and return an empty list\n",
    "            print(f\"Error occurred during search for query '{itm.query}': {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    # Run all the search tasks in parallel\n",
    "    search_tasks = [perform_search(itm) for itm in sub_queries]\n",
    "    search_responses = await asyncio.gather(*search_tasks)\n",
    "    \n",
    "    # Combine the results from all the responses\n",
    "    search_results = []\n",
    "    for response in search_responses:\n",
    "        search_results.extend(response)\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "# Code for adding Tavily Extract as a tool (found it more useful to use Tavily Extract in a separate node)\n",
    "# @tool(\"tavily_extract\", args_schema=TavilyExtractInput, return_direct=True)\n",
    "# async def tavily_extract(urls: TavilyExtractInput):\n",
    "#     \"\"\"Extract raw content from urls to gather additional information.\"\"\"\n",
    "#     try:\n",
    "#         response = await tavily_client.extract(urls=urls)\n",
    "#         return response['results']\n",
    "#     except Exception as e:\n",
    "#         # Handle any exceptions, log them, and return an empty list\n",
    "#         print(f\"Error occurred during extract: {str(e)}\")\n",
    "#         return []\n",
    "    \n",
    "\n",
    "tools = [tavily_search]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "tavily_client = AsyncTavilyClient()\n",
    "\n",
    "\n",
    "# Define an async custom research tool node to store Tavily's search results for improved processing and later on filtering\n",
    "async def tool_node(state: ResearchState):\n",
    "    docs = state.get('documents',{})\n",
    "    docs_str = \"\"\n",
    "    msgs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        new_docs = await tool.ainvoke(tool_call[\"args\"])\n",
    "        for doc in new_docs:\n",
    "            # Make sure that this document was not retrieved before\n",
    "            if not docs or doc['url'] not in docs:\n",
    "                docs[doc['url']] = doc\n",
    "                docs_str += json.dumps(doc)\n",
    "            # For Tavily Extract tool, checking if raw_content was retrieved a document\n",
    "            # if doc.get('raw_content', None) and doc['url'] in docs:\n",
    "            #     docs[doc['url']]['raw_content'] = doc['raw_content'] # add raw content retrieved by extract\n",
    "            #     docs_str += json.dumps(doc)\n",
    "        msgs.append(ToolMessage(content=f\"Found the following new documents/information: {docs_str}\", tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": msgs, \"documents\": docs}\n",
    "    \n",
    "# Invoke a model with research tools to gather data about the company  \n",
    "def research_model(state: ResearchState):\n",
    "    prompt = f\"\"\"Today's date is {datetime.now().strftime('%d/%m/%Y')}.\\n\n",
    "You are an expert researcher tasked with gathering information for a weekly report on recent developments in portfolio companies.\\n\n",
    "Your current objective is to gather documents about any significant events that occurred in the past week for the following company: {state['company']}.\\n\n",
    "The user has provided the following company keywords: {state['company_keywords']} to help you find documents relevant to the correct company.\\n     \n",
    "**Instructions:**\\n\n",
    "- Use the 'tavily_search' tool to search for relevant documents\n",
    "- Focus on gathering documents by making appropriate tool calls\n",
    "- If you believe you have gathered enough information, state 'I have gathered enough information and am ready to proceed.'\n",
    "\"\"\"\n",
    "    messages = state['messages'] + [SystemMessage(content=prompt)]\n",
    "    model = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        temperature=0,\n",
    "        max_tokens=2500,\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "    )\n",
    "    response = model.bind_tools(tools).invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "\n",
    "# Define the function that decides whether to continue research using tools or proceed to writing the report\n",
    "def should_continue(state: ResearchState) -> Literal[\"tools\", \"curate\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user with citations)\n",
    "    return \"curate\"\n",
    "\n",
    "async def select_and_process(state: ResearchState):\n",
    "    msg = \"Curating Documents ...\\n\"\n",
    "    prompt = f\"\"\"You are an expert researcher specializing in analyzing portfolio companies.\\n\n",
    "Your current task is to review a list of documents and select the most relevant URLs related to recent developments for the following company: {state['company']}.\\n\n",
    "Be aware that some documents may refer to other companies with similar or identical names, potentially leading to conflicting information.\\n\n",
    "Your objective is to choose the documents that pertain to the correct company and provide the most consistent and synchronized information, using the following keywords provided by the user to help identify the correct company as a guide:{state['company_keywords']}.\\n\"\"\"\n",
    "    # Optionally include exclusion keywords if provided by the user \n",
    "    if state['exclude_keywords'] != \"\":\n",
    "        prompt += f\"\"\"Additionally, if any form of the following exclusion words are present in the documents, do not include them and filter out those documents: {state['exclude_keywords']}.\\n\"\"\"\n",
    "    # Append the list of gathered documents to the prompt\n",
    "    prompt += f\"\"\"\\nHere is the list of documents gathered for your review:\\n{state['documents']}\\n\\n\"\"\"\n",
    "\n",
    "    # Use the model to filter documents and obtain relevant URLs structured as TavilyExtractInput\n",
    "    messages = [SystemMessage(content=prompt)]  \n",
    "    \n",
    "    model = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        temperature=0,\n",
    "        max_tokens=5000,\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "    )\n",
    "    relevant_urls = model.with_structured_output(TavilyExtractInput).invoke(messages)\n",
    "    \n",
    "    # Create a dictionary of relevant documents based on the URLs returned by the model\n",
    "    RAG_docs = {url: state['documents'][url] for url in relevant_urls.urls if url in state['documents']}\n",
    "\n",
    "    try:\n",
    "        # Extract raw content from the selected URLs using the Tavily client\n",
    "        response = await tavily_client.extract(urls=relevant_urls.urls)\n",
    "        \n",
    "        # Save the raw content into the RAG_docs dictionary for each URL\n",
    "        msg += \"Extracted raw content for:\\n\"\n",
    "        for itm in response['results']:\n",
    "            url = itm['url']\n",
    "            msg += f\"{url}\\n\" \n",
    "            raw_content = itm['raw_content']\n",
    "            RAG_docs[url]['raw_content'] = raw_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during Tavily Extract request\")\n",
    "        \n",
    "    msg += f\"ֿֿ\\n\\nState of RAG documents that will be used for the report:\\n\\n{RAG_docs}\"\n",
    "        \n",
    "    return {\"messages\": [AIMessage(content=msg)],\"RAG_docs\": RAG_docs}\n",
    "            \n",
    "# Define the function to write the report based on the retrieved documents.\n",
    "def write_report(state: ResearchState):\n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"Today's date is {datetime.now().strftime('%d/%m/%Y')}\\n.\n",
    "You are an expert researcher, writing a weekly report about recent events in portfolio companies.\\n\n",
    "Your task is to write an in-depth, well-written, and detailed report on the following company: {state['company']}. in markdown syntax\\n\n",
    "Here are all the documents you should base your answer on:\\n{state['RAG_docs']}\\n\"\"\" \n",
    "    # messages = [state['messages'][-1]] + [SystemMessage(content=prompt)] \n",
    "    # Create a system message with the constructed prompt (no need to include entire chat history)\n",
    "    messages = [SystemMessage(content=prompt)] \n",
    "    model = ChatGroq(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        temperature=0,\n",
    "        max_tokens=2500,\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "    )\n",
    "    response = model.with_structured_output(QuotedAnswer).invoke(messages)\n",
    "    full_report = response.answer\n",
    "    # Add Citations Section to the report\n",
    "    full_report += \"\\n\\n### Citations\\n\"\n",
    "    for citation in response.citations:\n",
    "        doc = state['RAG_docs'].get(citation.source_id)\n",
    "        full_report += f\"- [{doc.get('title',citation.source_id)}]({citation.source_id}): \\\"{citation.quote}\\\"\\n\"\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [AIMessage(content=f\"Generated Report:\\n{full_report}\")], \"report\": full_report}\n",
    "\n",
    "def generete_pdf(state: ResearchState):\n",
    "    directory = \"reports\"\n",
    "    file_name = f\"{state['company']} Weekly Report {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory)\n",
    "    msg = generate_pdf_from_md(state['report'], filename=f'{directory}/{file_name}.pdf')\n",
    "    return {\"messages\": [AIMessage(content=msg)]}\n",
    "\n",
    "# Define a graph\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"research\", research_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"curate\", select_and_process)\n",
    "workflow.add_node(\"write\", write_report)\n",
    "workflow.add_node(\"publish\", generete_pdf)\n",
    "# Set the entrypoint as route_query\n",
    "workflow.set_entry_point(\"research\")\n",
    "\n",
    "# Determine which node is called next\n",
    "workflow.add_conditional_edges(\n",
    "    \"research\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# Add a normal edge from `tools` to `research`.\n",
    "# This means that after `tools` is called, `research` node is called next in  order to determine if we should keep  or move to the 'curate' step\n",
    "workflow.add_edge(\"tools\", \"research\")\n",
    "workflow.add_edge(\"curate\",\"write\")\n",
    "workflow.add_edge(\"write\", \"publish\")  # Option in the future, to add another step and filter the documents retrieved using rerhank before writing the report\n",
    "workflow.add_edge(\"publish\", END)  # Option in the future, to add another step and filter the documents retrieved using rerhank before writing the report\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are an expert researcher ready to begin the information gathering process.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_gvgh)\n",
      " Call ID: call_gvgh\n",
      "  Args:\n",
      "    sub_queries: [{'query': 'Athena Intelligence data analyst', 'topic': 'general', 'days': 7, 'domains': ['www.athena-intelligence.com', 'www.crunchbase.com', 'www.linkedin.com']}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Found the following new documents/information: {\"title\": \"Athena Data Analyst Work, Jobs - 26 February, 2025 - Indeed\", \"url\": \"https://ph.indeed.com/q-athena-data-analyst-jobs.html\", \"content\": \"Apply to Athena Data Analyst jobs available on Indeed.com, the worlds largest job site.\", \"score\": 0.7606688, \"raw_content\": null}{\"title\": \"Athena Intelligence - Agentic Tools Directory\", \"url\": \"https://directory.composio.dev/listings/athena-intelligence/\", \"content\": \"Athena Intelligence - Agentic Tools Directory Agentic Tools Directory Athena Intelligence Athena Intelligence 24/7 Enterprise AI Data Analyst Data Analysis Enterprise Research 24/7 Enterprise AI Analyst for Seamless Automation Athena is a powerful AI analyst designed to automate repetitive tasks, freeing enterprise teams to focus on strategic initiatives. Built into the Olympus platform, Athena seamlessly integrates with familiar applications and enterprise data sources, enhancing workflows without altering existing habits. Seamless Integration: Works with existing enterprise data sources and applications to eliminate system navigation hurdles. Real-Time Collaboration: Supports collaborative work among colleagues and AI agents, with comprehensive activity logging and version history for transparency. ML research and product lab building intelligence Artificial Intelligence Business Intelligence Financial Intelligence Marketing Intelligence Sales Intelligence\", \"score\": 0.5611433, \"raw_content\": null}{\"title\": \"Athena Intelligence - AI Assistant for Data analysis | Find Your Agent\", \"url\": \"https://findyouragent.ai/agent/athena-intelligence\", \"content\": \"Athena Intelligence has developed Olympus, the world's first AI-native analytics platform designed for seamless human-machine collaboration. Operated by Athena, a 24/7 Enterprise AI Data Analyst, Olympus empowers organizations to unlock the full potential of their data and drive intelligent decision-making.\", \"score\": 0.48674685, \"raw_content\": null}{\"title\": \"PDF\", \"url\": \"https://groq.com/wp-content/uploads/2025/01/GroqCS25_athena-intelligence-letter.pdf\", \"content\": \"Groq Athena Intelligence: Real-time Inference for the Real World CUSTOMER USE CASE groq.com I 2 \\u00a92024 Groq, Inc. All rights reserved. Typically, a lot of the data analyst\\u2019s work is consumed by the mundane tasks of creating charts and slides; the Olympus co-pilot automates much of this work, freeing up analysts to spend more time on finding new insights and thinking about other strategic opportunities. As the expert analysts spend more time working with their Athena co-pilot, the co-pilot learns more about and ingests the analysts\\u2019 typical workflows. It\\u2019s just one example of how real-time AI, powered by Groq, will help organizations transform their business and tackle big challenges.\", \"score\": 0.4071218, \"raw_content\": null}{\"title\": \"Company | Athena Intelligence\", \"url\": \"https://www.athenaintel.com/company\", \"content\": \"Company | Athena Intelligence As a company with a loss function for customer value, we're at the forefront of this paradigm shift, engineering solutions that weave artificially intelligent models into the fabric of enterprise workflows. Athena Intelligence uniquely combines the AI-native platform Olympus with the AI agent Athena, enabling seamless transitions between co-pilot and agent workflows for unparalleled efficiency in\\u00a0data analysis. At Athena Intelligence, we're not just predicting the future of enterprise operations \\u2013 Forward Deployed Engineer  Full-Time New York, NY Product Engineer  Full-Time New York, NY Platform Engineer  Full-Time New York, NY We're funded by Village Global, Forward Deployed VC, Amjad Masad (CEO of Replit), and previous employees at OpenAI, Retool, Palantir, Scale AI, the US Navy, Nestle, and William Blair.\", \"score\": 0.40457857, \"raw_content\": null}{\"title\": \"Athena Intelligence 2025 Company Profile: Valuation, Funding ...\", \"url\": \"https://pitchbook.com/profiles/company/540721-09\", \"content\": \"failed\", \"score\": 0.3383618, \"raw_content\": null}{\"title\": \"Enterprise AI Data Analyst | AI Agent | Athena Intelligence\", \"url\": \"https://www.youtube.com/watch?v=CXmwYk5Hbig\", \"content\": \"About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy & Safety How YouTube works Test new features NFL Sunday Ticket Press Copyright\", \"score\": 0.28841922, \"raw_content\": null}{\"title\": \"About Athena | Data Analytics and Business Intelligence Platform\", \"url\": \"https://athena.global/about-us\", \"content\": \"Learn about Athena, the innovative business intelligence platform designed to empower growth. Discover our mission, values, and commitment to providing unparalleled business insights.\", \"score\": 0.27899823, \"raw_content\": null}{\"title\": \"AI-native analytics platform and artificial employee - Athena Intelligence\", \"url\": \"https://www.athenaintel.com/?ref=listmyai\", \"content\": \"Athena can be deployed into your company's Virtual Private Cloud (VPC), ensuring that your data always remains within your security perimeter. With this deployment model, all your sensitive data remains secure within your established security perimeter.\", \"score\": 0.2659262, \"raw_content\": null}{\"title\": \"Athena Intelligence | AI-native analytics platform and artificial employee\", \"url\": \"https://www.athenaintel.com/\", \"content\": \"Olympus Athena Deployment Options  Athena automates time-consuming tasks so teams can focus on the strategic work. Olympus is an AI-native platform, purpose built for Athena to operate. Research Synthesis Analyze consulting reports, market studies, and other data to gain actionable insights.Financial Research Automate financial filings download, enhancing strategic focus.Extract Financial Figures Extract and interpret financial guidance and management sentiment from earnings reports. Develop tailored content strategies based on market research and audience analysis. The Olympus platform encompasses all familiar knowledge work and applications that people have learned to operate, leaving the heavy lifting to\\u00a0Athena. Like a new team member, Athena's memory management system incorporates user preferences, business SOPs, enterprise objectives and strategic initiatives.\", \"score\": 0.24062778, \"raw_content\": null}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_8e82)\n",
      " Call ID: call_8e82\n",
      "  Args:\n",
      "    sub_queries: [{'query': 'Athena Intelligence data analyst', 'topic': 'news', 'days': 7, 'domains': ['www.bloomberg.com', 'www.reuters.com', 'www.cnbc.com']}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Found the following new documents/information: {\"url\": \"https://www.space.com/space-exploration/2-moon-landings-in-2-years-intuitive-machines-dreams-big-with-feb-26-launch-plan\", \"title\": \"2 moon landings in 2 years? Intuitive Machines dreams big with Feb. 26 launch plan - Space.com\", \"score\": 0.16742307, \"published_date\": \"Tue, 25 Feb 2025 22:41:47 GMT\", \"content\": \"The company's Athena lander is scheduled to launch toward the moon atop a SpaceX Falcon 9 rocket on Wednesday evening (Feb. 26). Intuitive Machines' second moon lander, named Athena, is set to launch atop a SpaceX Falcon 9 rocket on Feb. 26, 2025. CAPE CANAVERAL, Florida \\u2014 Intuitive Machines is poised to launch its second lunar lander in as many years, marking a significant milestone in commercial space exploration. The spacecraft, an Intuitive Machines Nova-C lander named Athena, is carrying a suite of lunar science instruments and technology demonstrations aimed at advancing our understanding of the moon's environment and its available resources. \\u2014 SpaceX launches 2 private lunar landers to the moon (video, photos)\"}{\"url\": \"https://www.scimag.news/news-en/134212/if-you-cant-make-it-to-florida-heres-how-to-watch-the-epic-moon-mission-online/\", \"title\": \"If You Can\\u2019t Make It to Florida, Here\\u2019s How to Watch the Epic Moon Mission Online - Science Magazine\", \"score\": 0.1452732, \"published_date\": \"Wed, 26 Feb 2025 17:31:18 GMT\", \"content\": \"Intuitive Machines Nova-C lander, named Athena, is scheduled to launch to the moon on February 26, 2025, from Kennedy Space Center. The mission, IM-2, aims to explore the lunar south pole\\u2019s shadowy craters for water ice, crucial for future lunar resource utilization. Discovering Moon\\u2019s Secrets: Intuitive Machines\\u2019 Nova-C Lander Athena Ready for Lunar Mission The upcoming launch of the Intuitive Machines Nova-C lander, named Athena, marks a significant step in space exploration. This mission, known as IM-2, is integral to advancing our understanding of the moon, particularly its supply of water ice in shadowy craters at the lunar south pole. \\u2013 Commercial Space Exploration: With private companies like SpaceX partnering with NASA, expect increased opportunities for commercial lunar missions.\"}{\"url\": \"https://www.bloomberg.com/news/videos/2025-02-21/bloomberg-intelligence-s-equity-market-minute-02-21-2025\", \"title\": \"Watch Bloomberg Intelligence's Equity Market Minute 02/21/2025 - Bloomberg\", \"score\": 0.09000516, \"published_date\": \"Fri, 21 Feb 2025 22:02:33 GMT\", \"content\": \"Watch Bloomberg Intelligence's Equity Market Minute 02/21/2025 - Bloomberg Bloomberg Tech At Bloomberg Bloomberg London Bloomberg Beta Bloomberg Terminal Bloomberg Tax Bloomberg Government Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Bloomberg Radio Tech At Bloomberg Bloomberg Terminal Bloomberg Tax Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Bloomberg Live on Bloomberg TV 00:00Welcome to the Equity Market Minute by Bloomberg Intelligence. Thanks so much for joining me for the Equity Market Minute by Bloomberg Intelligence. NOW PLAYING ### Bloomberg Intelligence's Equity Market Minute 02/21/2025  21:55 ### Bloomberg Markets 02/21/2025  Bloomberg Intelligence's Equity Market Minute 02/21/2025 Gina Martin Adams hosts this week's 'Equity Market Minute' by Bloomberg Intelligence. Bloomberg Technology\"}{\"url\": \"https://www.janes.com/osint-insights/defence-news/sea/strategic-mindset-sought-for-us-navy-shipbuilding-and-repair-says-gao-report\", \"title\": \"Strategic mindset sought for US Navy shipbuilding and repair, says GAO report - Jane's\", \"score\": 0.072959855, \"published_date\": \"Fri, 28 Feb 2025 11:23:50 GMT\", \"content\": \"Gain unlimited access to Janes news and more... Subscribe to Janes Login ### Solutions for the intelligence community Providing mission users with faster access to quality data to pre-empt threats and protect national security. Sea 28 February 2025 ### Strategic mindset sought for US Navy shipbuilding and repair, says GAO report Read Article By submitting this request, I agree with Janes terms of use * [ ]  Janes IntelTrak [ ]  About Janes [ ]  Receive the latest developments in defence and security as well as keep informed on Janes news and events. Janes Intelligence Summary [ ]  Janes Intelligence Summary I consent to receive Janes newsletters, product announcements and event invitations. Janes IntelTrak [ ]  About Janes [ ] \"}{\"url\": \"https://www.businessinsider.com/north-korea-fighting-fresh-troops-russia-kursk-south-korea-intel-2025-2\", \"title\": \"North Koreans are back to fighting alongside Russia, South Korean intelligence says - Business Insider\", \"score\": 0.06570681, \"published_date\": \"Thu, 27 Feb 2025 12:27:00 GMT\", \"content\": \"Published Time: 2025-02-27T12:27:32Z Finance Stocks Food Video Food Wars US edition North Koreans are back to fighting alongside Russia, South Korean intelligence says 2025-02-27T12:27:32Z North Korean troops have returned to fighting alongside Russia, South Korea's spy agency said. North Korean troops have returned to fighting alongside Russia, South Korea's spy agency said, following reports of earlier heavy losses. Last fall, Western and South Korean intelligence agencies said that Pyongyang had sent around 11,000-12,000 troops to fight in Kursk, the Russian region under partial occupation by Ukraine. russia ukraine war Recommended video Company News Stock quotes by finanzen.net International Editions\"}{\"url\": \"https://www.securityweek.com/1-5-billion-bybit-heist-linked-to-north-korean-hackers/\", \"title\": \"$1.5 Billion Bybit Heist Linked to North Korean Hackers - SecurityWeek\", \"score\": 0.05217782, \"published_date\": \"Mon, 24 Feb 2025 09:57:54 GMT\", \"content\": \"Published Time: 2025-02-24T09:52:34+00:00 $1.5 Billion Bybit Heist Linked to North Korean Hackers - SecurityWeek ICS: Data Breaches Vulnerabilities Threat Intelligence Cyber Insurance Data Protection Cyber Insurance Data Breaches Vulnerabilities Threat Intelligence Cyber Insurance Data Protection Cyber Insurance Multiple companies and experts have found evidence linking the massive Bybit cryptocurrency heist to North Korean hackers. Bybit Hack Drains $1.5 Billion From Cryptocurrency Exchange Freelance Software Developers in North Korean Malware Crosshairs Apple Pulls Advanced Data Protection for New UK Users Amid Backdoor Demand Webinar: CISO Forum 2025 Outlook \\u2013 Session 1 -------------------------------------------- Millions of uninformed users have flocked to DeepSeek and share personal information without considering security or privacy risks. Webcast Library Feedback/Contact Us Contact Us Unsubscribe at any time.\"}{\"url\": \"https://spacenews.com/second-intuitive-machines-lunar-lander-ready-for-launch/\", \"title\": \"Second Intuitive Machines lunar lander ready for launch - SpaceNews\", \"score\": 0.05165867, \"published_date\": \"Wed, 26 Feb 2025 12:33:32 GMT\", \"content\": \"Second Intuitive Machines lunar lander ready for launch - SpaceNews WASHINGTON \\u2014 The second lunar lander mission by Intuitive Machines is set to launch, taking to the moon NASA and commercial payloads as well as several rideshare spacecraft. At a Feb. 25 briefing, NASA and Intuitive Machines said they were still working towards a launch of the IM-2 mission on the evening of Feb. 26 on a Falcon 9 from the Kennedy Space Center. The IM-2 lander, called Athena by Intuitive Machines, is carrying NASA\\u2019s Polar Resources Ice Mining Experiment 1 (PRIME-1) payload as part of the agency\\u2019s Commercial Lunar Payload Services (CLPS) program. IM-2 is the second lunar lander mission for Intuitive Machines, launching just over a year after the IM-1 mission made to the lunar surface, although leaning on its side.\"}{\"url\": \"https://www.bloomberg.com/news/videos/2025-02-26/deepseek-spurs-investment-in-data-centers-flatt-says-video?srnd=phx-markets\", \"title\": \"Watch DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt - Bloomberg\", \"score\": 0.050410144, \"published_date\": \"Wed, 26 Feb 2025 15:47:15 GMT\", \"content\": \"Watch DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt - Bloomberg Bloomberg Tech At Bloomberg Bloomberg London Bloomberg Terminal Bloomberg Tax Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Tech At Bloomberg Bloomberg London Bloomberg Terminal Bloomberg Tax Bloomberg Environment Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Bloomberg Live on Bloomberg TV NOW PLAYING ### DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt  43:40 ### Bloomberg Brief 02/25/2025  DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt Brookfield Asset Management CEO Bruce Flatt says the success of Chinese startup DeepSeek means more data center capacity will be needed to handle the growing artificial intelligence workload. Bloomberg Technology\"}{\"url\": \"https://www.bloomberg.com/news/videos/2025-02-26/deepseek-spurs-investment-in-data-centers-flatt-says-video?srnd=all\", \"title\": \"Watch DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt - Bloomberg\", \"score\": 0.050410144, \"published_date\": \"Wed, 26 Feb 2025 15:47:15 GMT\", \"content\": \"Watch DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt - Bloomberg Bloomberg Tech At Bloomberg Bloomberg London Bloomberg Terminal Bloomberg Tax Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Tech At Bloomberg Bloomberg London Bloomberg Terminal Bloomberg Tax Bloomberg Environment Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Bloomberg Live on Bloomberg TV NOW PLAYING ### DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt  43:40 ### Bloomberg Brief 02/25/2025  DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt Brookfield Asset Management CEO Bruce Flatt says the success of Chinese startup DeepSeek means more data center capacity will be needed to handle the growing artificial intelligence workload. Bloomberg Technology\"}{\"url\": \"https://www.bloomberg.com/news/videos/2025-02-26/deepseek-spurs-investment-in-data-centers-flatt-says-video\", \"title\": \"Watch DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt - Bloomberg\", \"score\": 0.050410144, \"published_date\": \"Wed, 26 Feb 2025 15:47:15 GMT\", \"content\": \"Watch DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt - Bloomberg Bloomberg Tech At Bloomberg Bloomberg London Bloomberg Terminal Bloomberg Tax Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Tech At Bloomberg Bloomberg London Bloomberg Terminal Bloomberg Tax Bloomberg Environment Bloomberg Markets Bloomberg Technology Bloomberg Politics Bloomberg Opinion Bloomberg Businessweek Bloomberg Live Conferences Bloomberg Live on Bloomberg TV NOW PLAYING ### DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt  43:40 ### Bloomberg Brief 02/25/2025  DeepSeek Spurs Investment in Data Centers, Says Brookfield's Flatt Brookfield Asset Management CEO Bruce Flatt says the success of Chinese startup DeepSeek means more data center capacity will be needed to handle the growing artificial intelligence workload. Bloomberg Technology\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have gathered enough information and am ready to proceed.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': ' {\"urls\": [\"https://directory.composio.dev/listings/athena-intelligence/\", \"https://findyouragent.ai/agent/athena-intelligence\", \"https://groq.com/wp-content/uploads/2025/01/GroqCS25_athena-intelligence-letter.pdf\", \"https://www.athenaintel.com/company\", \"https://www.athenaintel.com/?ref=listmyai\", \"https://www.athenaintel.com/\", \"https://www.space.com/space-exploration/2-moon-landings-in-2-years-intuitive-machines-dreams-big-with-feb-26-launch-plan\", \"https://www.scimag.news/news-en/134212/if-you-cant-make-it-to-florida-heres-how-to-watch-the-epic-moon-mission-online/\", \"https://spacenews.com/second-intuitive-machines-lunar-lander-ready-for-launch/\"]}'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# You may uncomment your_additional_guidelines and HumanMessage and update the content with some guidelines of your own\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# your_additional_guidelines=f\"Note that the {company} is ... / focus on ....\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an expert researcher ready to begin the information gathering process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# ,HumanMessage(content=your_additional_guidelines)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m\"\u001b[39m: company, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: company_keywords, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_keywords, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m:messages}, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     13\u001b[0m     message \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2274\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2274\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   2275\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2276\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2277\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2278\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2279\u001b[0m     ):\n\u001b[1;32m   2280\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2281\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   2282\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langgraph/pregel/runner.py:444\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    442\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[1;32m    445\u001b[0m         t,\n\u001b[1;32m    446\u001b[0m         retry_policy,\n\u001b[1;32m    447\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    448\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    449\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[1;32m    450\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[1;32m    451\u001b[0m         },\n\u001b[1;32m    452\u001b[0m     )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:128\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langgraph/utils/runnable.py:583\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    580\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langgraph/utils/runnable.py:373\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[4], line 175\u001b[0m, in \u001b[0;36mselect_and_process\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    166\u001b[0m messages \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39mprompt)]  \n\u001b[1;32m    168\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatGroq(\n\u001b[1;32m    169\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1-8b-instant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    170\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 175\u001b[0m relevant_urls \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTavilyExtractInput\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Create a dictionary of relevant documents based on the URLs returned by the model\u001b[39;00m\n\u001b[1;32m    178\u001b[0m RAG_docs \u001b[38;5;241m=\u001b[39m {url: state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][url] \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m relevant_urls\u001b[38;5;241m.\u001b[39murls \u001b[38;5;28;01mif\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5360\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5355\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5356\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5357\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/langchain_groq/chat_models.py:480\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    476\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    479\u001b[0m }\n\u001b[0;32m--> 480\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/resources/chat/completions.py:322\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    956\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source-codes/ AI-agent-based-Deep-Research/venv/lib/python3.10/site-packages/groq/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': ' {\"urls\": [\"https://directory.composio.dev/listings/athena-intelligence/\", \"https://findyouragent.ai/agent/athena-intelligence\", \"https://groq.com/wp-content/uploads/2025/01/GroqCS25_athena-intelligence-letter.pdf\", \"https://www.athenaintel.com/company\", \"https://www.athenaintel.com/?ref=listmyai\", \"https://www.athenaintel.com/\", \"https://www.space.com/space-exploration/2-moon-landings-in-2-years-intuitive-machines-dreams-big-with-feb-26-launch-plan\", \"https://www.scimag.news/news-en/134212/if-you-cant-make-it-to-florida-heres-how-to-watch-the-epic-moon-mission-online/\", \"https://spacenews.com/second-intuitive-machines-lunar-lander-ready-for-launch/\"]}'}}"
     ]
    }
   ],
   "source": [
    "company = \"Athena Intelligence\"\n",
    "company_keywords = \"data analyst\"\n",
    "# (Optional) exclude_keywords: Use this field when you need to differentiate the company from others with the same name in a different industry\n",
    "# or when you want to exclude specific types of documents or information. Leave it as an empty string (\"\") if not needed.\n",
    "exclude_keywords = \"wildfire\"\n",
    "# You may uncomment your_additional_guidelines and HumanMessage and update the content with some guidelines of your own\n",
    "# your_additional_guidelines=f\"Note that the {company} is ... / focus on ....\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert researcher ready to begin the information gathering process.\")\n",
    "    # ,HumanMessage(content=your_additional_guidelines)\n",
    "]\n",
    "async for s in app.astream({\"company\": company, \"company_keywords\": company_keywords, \"exclude_keywords\": exclude_keywords, \"messages\":messages}, stream_mode=\"values\"):\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
